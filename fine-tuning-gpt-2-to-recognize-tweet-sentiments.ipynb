{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sharooqfarzeenak/fine-tuning-gpt-2-to-recognize-tweet-sentiments?scriptVersionId=211178032\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# !pip install datasets pandas transformers evaluate numpy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T10:35:57.728107Z","iopub.execute_input":"2024-12-04T10:35:57.728672Z","iopub.status.idle":"2024-12-04T10:36:10.518775Z","shell.execute_reply.started":"2024-12-04T10:35:57.728616Z","shell.execute_reply":"2024-12-04T10:36:10.516551Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.1.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.3)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.46.3)\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (18.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.26.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.6.2)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\n","output_type":"stream"}],"execution_count":61},{"cell_type":"markdown","source":"## Loading and analyzing the dataset","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T10:21:53.713713Z","iopub.execute_input":"2024-12-04T10:21:53.714125Z","iopub.status.idle":"2024-12-04T10:21:53.72288Z","shell.execute_reply.started":"2024-12-04T10:21:53.714067Z","shell.execute_reply":"2024-12-04T10:21:53.721729Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"dataset = load_dataset(\"mteb/tweet_sentiment_extraction\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T10:21:53.724286Z","iopub.execute_input":"2024-12-04T10:21:53.724607Z","iopub.status.idle":"2024-12-04T10:21:54.544212Z","shell.execute_reply.started":"2024-12-04T10:21:53.724573Z","shell.execute_reply":"2024-12-04T10:21:54.543162Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T10:21:54.546208Z","iopub.execute_input":"2024-12-04T10:21:54.546635Z","iopub.status.idle":"2024-12-04T10:21:54.553602Z","shell.execute_reply.started":"2024-12-04T10:21:54.546599Z","shell.execute_reply":"2024-12-04T10:21:54.552484Z"}},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'text', 'label', 'label_text'],\n        num_rows: 27481\n    })\n    test: Dataset({\n        features: ['id', 'text', 'label', 'label_text'],\n        num_rows: 3534\n    })\n})"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"dataset['train']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T10:21:54.555018Z","iopub.execute_input":"2024-12-04T10:21:54.555424Z","iopub.status.idle":"2024-12-04T10:21:54.567634Z","shell.execute_reply.started":"2024-12-04T10:21:54.555383Z","shell.execute_reply":"2024-12-04T10:21:54.566464Z"}},"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['id', 'text', 'label', 'label_text'],\n    num_rows: 27481\n})"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.DataFrame(dataset['train'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T10:21:54.57046Z","iopub.execute_input":"2024-12-04T10:21:54.570923Z","iopub.status.idle":"2024-12-04T10:21:55.768606Z","shell.execute_reply.started":"2024-12-04T10:21:54.570874Z","shell.execute_reply":"2024-12-04T10:21:55.76755Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T10:21:55.76966Z","iopub.execute_input":"2024-12-04T10:21:55.76998Z","iopub.status.idle":"2024-12-04T10:21:55.781576Z","shell.execute_reply.started":"2024-12-04T10:21:55.769949Z","shell.execute_reply":"2024-12-04T10:21:55.780397Z"}},"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"           id                                               text  label  \\\n0  cb774db0d1                I`d have responded, if I were going      1   \n1  549e992a42      Sooo SAD I will miss you here in San Diego!!!      0   \n2  088c60f138                          my boss is bullying me...      0   \n3  9642c003ef                     what interview! leave me alone      0   \n4  358bd9e861   Sons of ****, why couldn`t they put them on t...      0   \n\n  label_text  \n0    neutral  \n1   negative  \n2   negative  \n3   negative  \n4   negative  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>label</th>\n      <th>label_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>cb774db0d1</td>\n      <td>I`d have responded, if I were going</td>\n      <td>1</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>549e992a42</td>\n      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n      <td>0</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>088c60f138</td>\n      <td>my boss is bullying me...</td>\n      <td>0</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9642c003ef</td>\n      <td>what interview! leave me alone</td>\n      <td>0</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>358bd9e861</td>\n      <td>Sons of ****, why couldn`t they put them on t...</td>\n      <td>0</td>\n      <td>negative</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T10:21:55.783077Z","iopub.execute_input":"2024-12-04T10:21:55.783518Z","iopub.status.idle":"2024-12-04T10:21:55.795485Z","shell.execute_reply.started":"2024-12-04T10:21:55.783484Z","shell.execute_reply":"2024-12-04T10:21:55.794364Z"}},"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"Index(['id', 'text', 'label', 'label_text'], dtype='object')"},"metadata":{}}],"execution_count":42},{"cell_type":"markdown","source":"## Tokenizing","metadata":{}},{"cell_type":"code","source":"from transformers import GPT2Tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T10:21:55.796883Z","iopub.execute_input":"2024-12-04T10:21:55.797368Z","iopub.status.idle":"2024-12-04T10:21:55.810667Z","shell.execute_reply.started":"2024-12-04T10:21:55.797304Z","shell.execute_reply":"2024-12-04T10:21:55.809433Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"# Loading the dataset to train our model\ndataset = load_dataset(\"mteb/tweet_sentiment_extraction\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T10:27:22.71745Z","iopub.execute_input":"2024-12-04T10:27:22.717884Z","iopub.status.idle":"2024-12-04T10:27:23.652322Z","shell.execute_reply.started":"2024-12-04T10:27:22.717851Z","shell.execute_reply":"2024-12-04T10:27:23.650961Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"# Initializing the gpt-2 tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T10:21:55.811955Z","iopub.execute_input":"2024-12-04T10:21:55.81236Z","iopub.status.idle":"2024-12-04T10:21:56.080149Z","shell.execute_reply.started":"2024-12-04T10:21:55.812325Z","shell.execute_reply":"2024-12-04T10:21:56.078956Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"def tokenize(element):\n   return tokenizer(element[\"text\"], padding=\"max_length\", truncation=True)\n\ntokenized_data = dataset.map(tokenize, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T10:28:18.049442Z","iopub.execute_input":"2024-12-04T10:28:18.050431Z","iopub.status.idle":"2024-12-04T10:28:36.686532Z","shell.execute_reply.started":"2024-12-04T10:28:18.050383Z","shell.execute_reply":"2024-12-04T10:28:36.685312Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/27481 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54c8b187daa84581a2f9a346871d69ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3534 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0dfb2a4ddb77424683148d5b05268071"}},"metadata":{}}],"execution_count":48},{"cell_type":"code","source":"tokenized_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T10:28:40.989137Z","iopub.execute_input":"2024-12-04T10:28:40.98953Z","iopub.status.idle":"2024-12-04T10:28:40.997281Z","shell.execute_reply.started":"2024-12-04T10:28:40.989498Z","shell.execute_reply":"2024-12-04T10:28:40.996012Z"}},"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'text', 'label', 'label_text', 'input_ids', 'attention_mask'],\n        num_rows: 27481\n    })\n    test: Dataset({\n        features: ['id', 'text', 'label', 'label_text', 'input_ids', 'attention_mask'],\n        num_rows: 3534\n    })\n})"},"metadata":{}}],"execution_count":49},{"cell_type":"code","source":"# Extracting train and eval sets from tokenized_data, which is a Dataset dictionary\n# and trimming the dataset to 1000 because of performance limitations\ntokenized_train_dataset = tokenized_data[\"train\"].shuffle(seed=42).select(range(1000))\ntokenized_eval_dataset = tokenized_data[\"test\"].shuffle(seed=42).select(range(1000))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T10:29:12.387277Z","iopub.execute_input":"2024-12-04T10:29:12.387703Z","iopub.status.idle":"2024-12-04T10:29:12.40596Z","shell.execute_reply.started":"2024-12-04T10:29:12.387666Z","shell.execute_reply":"2024-12-04T10:29:12.404826Z"}},"outputs":[],"execution_count":51},{"cell_type":"markdown","source":"## Initializing our model3)","metadata":{}},{"cell_type":"code","source":"# Number of labels in our classification\nnum_labels = len(df['label'].unique())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T10:33:01.870095Z","iopub.execute_input":"2024-12-04T10:33:01.870923Z","iopub.status.idle":"2024-12-04T10:33:01.876834Z","shell.execute_reply.started":"2024-12-04T10:33:01.870881Z","shell.execute_reply":"2024-12-04T10:33:01.875523Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"from transformers import GPT2ForSequenceClassification\n\n# Loading GPT-2 model for fine-tuning\n# number of labels = number of emotions in our dataset\nmodel = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=num_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T10:31:18.105122Z","iopub.execute_input":"2024-12-04T10:31:18.106133Z","iopub.status.idle":"2024-12-04T10:31:37.62284Z","shell.execute_reply.started":"2024-12-04T10:31:18.106089Z","shell.execute_reply":"2024-12-04T10:31:37.621607Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87b9a7eca6fa4498ad1b223ba033ecc4"}},"metadata":{}},{"name":"stderr","text":"Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T10:33:51.721041Z","iopub.execute_input":"2024-12-04T10:33:51.721541Z","iopub.status.idle":"2024-12-04T10:33:51.730556Z","shell.execute_reply.started":"2024-12-04T10:33:51.721486Z","shell.execute_reply":"2024-12-04T10:33:51.729072Z"}},"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"GPT2ForSequenceClassification(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2SdpaAttention(\n          (c_attn): Conv1D(nf=2304, nx=768)\n          (c_proj): Conv1D(nf=768, nx=768)\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D(nf=3072, nx=768)\n          (c_proj): Conv1D(nf=768, nx=3072)\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (score): Linear(in_features=768, out_features=3, bias=False)\n)"},"metadata":{}}],"execution_count":59},{"cell_type":"markdown","source":"## Creating evaluation method to pass on to trainer","metadata":{}},{"cell_type":"code","source":"import evaluate\nimport numpy as np\n\nmetric = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n   logits, labels = eval_pred\n   predictions = np.argmax(logits, axis=-1)\n   return metric.compute(predictions=predictions, references=labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T10:36:12.346905Z","iopub.execute_input":"2024-12-04T10:36:12.347387Z","iopub.status.idle":"2024-12-04T10:36:13.612227Z","shell.execute_reply.started":"2024-12-04T10:36:12.347346Z","shell.execute_reply":"2024-12-04T10:36:13.611054Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"884e8e89a8c443748441abdfced2021d"}},"metadata":{}}],"execution_count":62},{"cell_type":"markdown","source":"## Fine-Tuning","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T10:42:22.55431Z","iopub.execute_input":"2024-12-04T10:42:22.555403Z","iopub.status.idle":"2024-12-04T10:42:23.971006Z","shell.execute_reply.started":"2024-12-04T10:42:22.555356Z","shell.execute_reply":"2024-12-04T10:42:23.969938Z"}},"outputs":[],"execution_count":63},{"cell_type":"markdown","source":"#### TrainingArguments\r\n\r\n##### Parameters\r\n\r\noutput_dir (str) – The output directory where the model predictions and checkpoints will be written.\r\n\r\noverwrite_output_dir (bool, optional, defaults to False) – If True, overwrite the content of the output directory. Use this to continue training if output_dir points to a checkpoint directory.\r\n\r\ndo_train (bool, optional, defaults to False) – Whether to run training or not.\r\n\r\ndo_eval (bool, optional, defaults to False) – Whether to run evaluation on the dev set or not.\r\n\r\ndo_predict (bool, optional, defaults to False) – Whether to run predictions on the test set or not.\r\n\r\nevaluate_during_training (bool, optional, defaults to False) – Whether to run evaluation during training at each logging step or not.\r\n\r\nper_device_train_batch_size (int, optional, defaults to 8) – The batch size per GPU/TPU core/CPU for training.\r\n\r\nper_device_eval_batch_size (int, optional, defaults to 8) – The batch size per GPU/TPU core/CPU for evaluation.\r\n\r\ngradient_accumulation_steps – (int, optional, defaults to 1): Number of updates steps to accumulate the gradients for, before performing a backward/update pass.\r\n\r\nlearning_rate (float, optional, defaults to 5e-5) – The initial learning rate for Adam.\r\n\r\nweight_decay (float, optional, defaults to 0) – The weight decay to apply (if not zero).\r\n\r\nadam_epsilon (float, optional, defaults to 1e-8) – Epsilon for the Adam optimizer.\r\n\r\nmax_grad_norm (float, optional, defaults to 1.0) – Maximum gradient norm (for gradient clipping).\r\n\r\nnum_train_epochs (float, optional, defaults to 3.0) – Total number of training epochs to perform.\r\n\r\nmax_steps (int, optional, defaults to -1) – If set to a positive number, the total number of training steps to perform. Overrides num_train_epochs.\r\n\r\nwarmup_steps (int, optional, defaults to 0) – Number of steps used for a linear warmup from 0 to learning_rate.\r\n\r\nlogging_dir (str, optional) – Tensorboard log directory. Will default to runs/**CURRENT_DATETIME_HOSTNAME**.\r\n\r\nlogging_first_step (bool, optional, defaults to False) – Wheter to log and evalulate the first global_step or not.\r\n\r\nlogging_steps (int, optional, defaults to 500) – Number of update steps between two logs.\r\n\r\nsave_steps (int, optional, defaults to 500) – Number of updates steps before two checkpoint saves.\r\n\r\nsave_total_limit (int, optional) – If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in output_dir.\r\n\r\nno_cuda (bool, optional, defaults to False) – Wherher to not use CUDA even when it is available or not.\r\n\r\nseed (int, optional, defaults to 42) – Random seed for initialization.\r\n\r\nfp16 (bool, optional, defaults to False) – Whether to use 16-bit (mixed) precision training (through NVIDIA apex) instead of 32-bit training.\r\n\r\nfp16_opt_level (str, optional, defaults to ‘O1’) – For fp16 training, apex AMP optimization level selected in [‘O0’, ‘O1’, ‘O2’, and ‘O3’]. See details on the apex documentation.\r\n\r\nlocal_rank (int, optional, defaults to -1) – During distributed training, the rank of the process.\r\n\r\ntpu_num_cores (int, optional) – When training on TPU, the mumber of TPU cores (automatically passed by launcher script).\r\n\r\ndebug (bool, optional, defaults to False) – When training on TPU, whether to print debug metrics or not.\r\n\r\ndataloader_drop_last (bool, optional, defaults to False) – Whether to drop the last incomplete batch (if the length of the dataset is not divisible by the batch size) or not.\r\n\r\neval_steps (int, optional, defaults to 1000) – Number of update steps between two evaluations.\r\n\r\npast_index (int, optional, defaults to -1) – Some models like TransformerXL or :doc`XLNet <../model_doc/xlnet>` can make use of the past hidden states for their predictions. If this argument is set to a positive int, the Trainer will use the corresponding output (usually index 2) as the past state and feed it to the model at the next training step under the keyword argument mems.del at the next training step under the keyword argument mems.","metadata":{}},{"cell_type":"code","source":"# Configuring the trainer\ntraining_args = TrainingArguments(\n   output_dir=\"test_trainer\",\n   #evaluation_strategy=\"epoch\",\n   per_device_train_batch_size=1,  # Reduce batch size here\n   per_device_eval_batch_size=1,    # Optionally, reduce for evaluation as well\n   gradient_accumulation_steps=4\n   )\n\n\ntrainer = Trainer(\n   model=model,\n   args=training_args,\n   train_dataset=tokenized_train_dataset,\n   eval_dataset=tokenized_eval_dataset,\n   compute_metrics=compute_metrics\n\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T10:43:22.056371Z","iopub.execute_input":"2024-12-04T10:43:22.05676Z","iopub.status.idle":"2024-12-04T10:43:23.571901Z","shell.execute_reply.started":"2024-12-04T10:43:22.056727Z","shell.execute_reply":"2024-12-04T10:43:23.570724Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T10:43:28.386435Z","iopub.execute_input":"2024-12-04T10:43:28.386825Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241204_104655-cdlwu0qo</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/sharooqfarzeen-other/huggingface/runs/cdlwu0qo' target=\"_blank\">test_trainer</a></strong> to <a href='https://wandb.ai/sharooqfarzeen-other/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/sharooqfarzeen-other/huggingface' target=\"_blank\">https://wandb.ai/sharooqfarzeen-other/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/sharooqfarzeen-other/huggingface/runs/cdlwu0qo' target=\"_blank\">https://wandb.ai/sharooqfarzeen-other/huggingface/runs/cdlwu0qo</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='45' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 45/750 25:06 < 6:51:38, 0.03 it/s, Epoch 0.18/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":"## Model Evaluation","metadata":{}},{"cell_type":"code","source":"import evaluate\n\ntrainer.evaluate()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}